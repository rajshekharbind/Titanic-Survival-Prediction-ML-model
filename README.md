# Titanic-Survival-Prediction-ML-model

ğŸš¢ Titanic Survival Prediction using Machine Learning Pipeline
ğŸ“Œ Project Overview

The Titanic Survival Prediction project aims to predict whether a passenger survived the Titanic disaster using supervised machine learning.
This project is implemented using Scikit-Learn Pipelines, ensuring a clean, modular, scalable, and production-ready workflow.

The pipeline automates:

Data preprocessing

Feature engineering

Feature selection

Model training

Hyperparameter tuning

Model evaluation
ğŸ¯ Objective

To build a robust ML model that predicts passenger survival based on demographic and travel-related features while following best ML engineering practices.

ğŸ“‚ Dataset Information

Dataset: Titanic Dataset (Kaggle)

Target Variable: Survived

0 â†’ Did not survive

1 â†’ Survived

âŒ Dropped Columns
PassengerId, Name, Ticket, Cabin


These columns either contain excessive missing values or do not contribute meaningfully to prediction.

ğŸ§° Libraries & Tools Used
ğŸ”¹ Core Libraries
numpy
pandas
scikit-learn

ğŸ”¹ Scikit-Learn Modules

Data Splitting: train_test_split

Pipelines: Pipeline, make_pipeline

Preprocessing:

ColumnTransformer

SimpleImputer

OneHotEncoder

MinMaxScaler

Feature Selection: SelectKBest, chi2

Model: DecisionTreeClassifier

Evaluation:

accuracy_score

cross_val_score

Hyperparameter Tuning: GridSearchCV

ğŸ” End-to-End Machine Learning Pipeline
ğŸ§  Pipeline Process Diagram
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Titanic Data  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Train / Test Split (80/20)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Numerical Featuresâ”‚        â”‚ Categorical Features â”‚
â”‚ (Age, Fare, etc.) â”‚        â”‚ (Sex, Embarked, etc.)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                              â”‚
          â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mean Imputation  â”‚        â”‚ Most-Frequent Imputer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                              â”‚
          â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinMax Scaler    â”‚        â”‚ One-Hot Encoding       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  ColumnTransformer       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ SelectKBest (Chi-Square) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Decision Tree Classifier â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Model Prediction Output  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ›  Pipeline Stages Explained
1ï¸âƒ£ Train-Test Split

80% Training

20% Testing

Ensures unbiased model evaluation.

2ï¸âƒ£ Data Preprocessing
ğŸ”¹ Numerical Features

Missing values â†’ Mean Imputation

Scaling â†’ MinMaxScaler (0â€“1 range)

ğŸ”¹ Categorical Features

Missing values â†’ Most Frequent

Encoding â†’ OneHotEncoder

3ï¸âƒ£ ColumnTransformer

Combines numerical and categorical preprocessing into a single unified step, ensuring clean data flow.

4ï¸âƒ£ Feature Selection
SelectKBest(score_func=chi2)


Selects the most relevant features

Reduces noise and overfitting

5ï¸âƒ£ Model Training
DecisionTreeClassifier


Interpretable model

Handles non-linear patterns efficiently

ğŸ“Š Model Evaluation
âœ… Accuracy Score

Evaluated on test data using:

accuracy_score(y_test, y_pred)

ğŸ” Cross Validation
cross_val_score(pipe, X_train, y_train, cv=5)


5-fold cross validation

Improves generalization reliability

ğŸ” Hyperparameter Tuning
GridSearchCV Parameters
selectkbest__k = [5, 8, 10]
decisiontreeclassifier__max_depth = [3, 5, None]

Benefits

Automatically finds best model

Prevents underfitting & overfitting

Improves accuracy consistency

ğŸ† Final Results

Test Accuracy: ~ Optimized via GridSearchCV

Cross-Validated Accuracy: Mean of 5 folds

Pipeline ensures no data leakage

â–¶ï¸ How to Run the Project
pip install numpy pandas scikit-learn
jupyter notebook titanic-using-pipeline.ipynb
